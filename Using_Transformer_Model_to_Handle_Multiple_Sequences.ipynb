{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using Transformer Model to Handle Multiple Sequences.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9SddYhStDuQLD3FoUmb06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruthgn/HF/blob/main/Using_Transformer_Model_to_Handle_Multiple_Sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After exploring the simplest of uses cases--doing inference on a single sequence of a small length--some questions emerge already:\n",
        "- How do we handle multiple sequences?\n",
        "- How do we handle multiple sequences of different lenghts?\n",
        "- Are vocabulary indices the only inputs that allow a model to work well?\n",
        "- Is there such a thing as too long a sequence?\n",
        "\n",
        "Let's see what kinds of problems these questions pose, and how we can solve them using the HF Transformers API."
      ],
      "metadata": {
        "id": "oNpu4ngi55YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUSURJk1S67Y",
        "outputId": "87069e2f-0557-4e80-977a-87b80e39fa3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.11.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.96)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models expect a batch of inputs"
      ],
      "metadata": {
        "id": "k_k00HQt6Sok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"Water is the key to life on earth.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Remember to add new dimension (by using \"[]\")\n",
        "# when sending a single sequence to the model\n",
        "# because Transformer models expect \n",
        "# multiple sentences by default\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOvhpf946OGD",
        "outputId": "bb876a91-d29d-4ce4-abf8-103672c1f1e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[2300, 2003, 1996, 3145, 2000, 2166, 2006, 3011, 1012]])\n",
            "Logits: tensor([[-3.7894,  4.0509]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Batching_ is the act of sending **multiple sentences** through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence.\n",
        "\n",
        "This is a batch of two identical sequences:"
      ],
      "metadata": {
        "id": "gftbTvL0T3WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [ids, ids]"
      ],
      "metadata": {
        "id": "0EGZDUS_UEKv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we convert this `batched_ids` list into a tensor and pass it through a model, we'll obtain the same logits as before (but twice)."
      ],
      "metadata": {
        "id": "umeKSUgvUVhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor(batched_ids)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rveTb9VJUSWc",
        "outputId": "a756598e-84e1-4a7c-f2ff-80e486f42feb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[2300, 2003, 1996, 3145, 2000, 2166, 2006, 3011, 1012],\n",
            "        [2300, 2003, 1996, 3145, 2000, 2166, 2006, 3011, 1012]])\n",
            "Logits tensor([[-3.7894,  4.0509],\n",
            "        [-3.7894,  4.0509]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding the inputs"
      ],
      "metadata": {
        "id": "3i-oIaFmU9Yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use *padding* to make our tensors have a rectangular shape--essentially making sure all our sentences have the same length by adding a special word called the *padding token* to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words."
      ],
      "metadata": {
        "id": "BJxCbuZ-VKGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: A key feature of Transformer models is attention layers that _contextualize_ each token. **These will take into account the padding tokens since they attend to all of the tokens of a sequence--we need to tell those attention layers to ignore the padding tokens.** This can be done by using an attention mask."
      ],
      "metadata": {
        "id": "tdl9LyRhYkgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention masks"
      ],
      "metadata": {
        "id": "zcO4DgUVYJRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Attention masks* are tensors with the exact sama shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attentded to (i.e, they should be ignored by the attention layers of the model).\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "iD28NbGNYTM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Apply the tokenization manually on two sentences, batch them together using the padding token, then create the proper attention mask. Make sure to obtain the same results when going through the model!"
      ],
      "metadata": {
        "id": "XFueRGFBZkCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Sentences\n",
        "sequence1 = \"Iâ€™ve been waiting for a HuggingFace course my whole life.\"\n",
        "sequence2 = \"I hate this so much!\"\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence1)\n",
        "sequence1_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence2)\n",
        "sequence2_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "batched_ids = [sequence1_ids, sequence2_ids]\n",
        "\n",
        "print(batched_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iMi3EOjU5EV",
        "outputId": "2f3364d0-d860-4b0b-97c1-9e622d788e4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119], [146, 4819, 1142, 1177, 1277, 106]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "batched_ids = [[146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119], \n",
        "               [146, 4819, 1142, 1177, 1277, 106,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id,\n",
        "                tokenizer.pad_token_id]\n",
        "           ]\n",
        "\n",
        "# Attention mask\n",
        "attention_mask = [\n",
        "                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXuAG69nroOo",
        "outputId": "8a8c47ff-3c64-4edd-827c-6269a6d741df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.6594, -1.4647],\n",
            "        [ 1.4024, -1.2253]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "print(model(torch.tensor([sequence1_ids])).logits)\n",
        "print(model(torch.tensor([sequence2_ids])).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj1ERpxoyC5V",
        "outputId": "573449b6-525c-402b-e601-334631d7a31c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.6594, -1.4647]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.4024, -1.2253]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Note: With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. The solutions to this problem are: 1.) Use a model with a longer supporter sequence length (e.g.: Longformer, LED). 2.) Truncate your sequences (specify the `max_sequence_length` parameter)_"
      ],
      "metadata": {
        "id": "YV1WqE7OaPO6"
      }
    }
  ]
}